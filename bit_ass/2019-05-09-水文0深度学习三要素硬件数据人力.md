---
layout: default
---
# 【水文0】深度学习三要素：硬件、数据、人力

 [*Link:*](https://zhuanlan.zhihu.com/p/65247452)

由于最近结束了上一段工作，便决定重新思考和整理总结，都是自己的私人观点，并且都是水文。

## 深度学习事实上降低了工程门槛  
深度学习（DL，限制在基于神经网络的方法）技术领域近几年出现了很繁荣的现象。我想这里面有一个很重要的原因： 


> DL把几个重要任务(图像、语音一类)工程方面的门槛大幅度拉低了。

**为什么呢？** 

* **神经网络（NN）提供的通用拟合能力**

NN几乎直接提供了一个universal approximator，人类的先验知识在这里变得越来越不重要。以前需要专业人员辛辛苦苦花几年时间搞特征，现在DL直接怼就可以，而且怼的比专业人员效果还好。 搞得所有领域都NLP经典段子那样，少点语言专家帮忙，模型还能更好点。

这样，问题就从“搞特征”变成了“怼数据”。“怼数据”要容易很多，对专业人员的需求也降低了很多，标注员的培训是容易得多的。 

  


* **软、硬件环境的迅速建立和完善**

硬件上，GPU基本是NVIDIA一家撑起了90%的天，从HPC到嵌入式，老黄基本包圆了，硬件方案简（you）单（qian）明（jiu）了（xing)。后续，又有各个公司进一步的加入，提供功耗比优势的硬件。训练方案基本统一，同时部署方案又有很多选择。 （不得不佩服NVIDIA的布局能力和运气，十几年前就开始推CUDA了。）

软件上：

1. **DL的通用性直接带来的是软件工具通用性**。Tensorflow、PyTorch、MXNet以及其他（其实，MXNet放其他里也不冤）统统都可以用在各种任务上。而且，这些平台都直接免费+开源，随便用，还附送一些成熟算法的集成，看起来就是全世界的资本家都在争先恐后的做慈善。
2. **由于DL的通用性，部署的工具链也容易构建。**其实就是权值转换和那几个基本操作（矩阵操作、激活函数）的指令集的对应，硬件商都愿意提供，当然，提供的工具链完成度怎么样另说，但是都是达到了又不是不能用的级别。

  


* **开放、开源的社区环境**

最新的论文都是挂arXiv上随便看。State of the art的论文基本都有开源实现。拿来就用，快速起步。商业公司会有技术保留，但是也不影响自用开源方案在自己业务的场景里面，技术保留的那几个点优势在实际的应用中表现并不明显（毕竟实际应用是要考虑场景和边际效用的）。核心技术的大致思路都是那样，不会有显著差异。

这上面每一点都是大规模降低工程化门槛的。三点凑一起，一个人撸起袖子就能干，准备好了数据，写个符合场景的原型确实可以到分分钟了。 免责声明：成熟的解决方案分分钟的难度还是比较大的。



---

## 硬件、数据、人力 三要素  
这基本就是DL的三大件，按照准备的难度由易到难： 

  


* **硬件最简单**（大规模集群有门槛）

毕竟有钱就能搞到，DGX-2也就四十万刀么...科科，我们换个话题。便宜的方案也有啊，40万刀能堆一房间了。 不谈大规模集群的硬件摸索过程，小厂的硬件到位是很快的。

最不济，小作坊买台四卡机器，今天下单，明天京东小哥就送到了。

  


* **人力要比设备难度大一点**

毕竟团队的组建是需要时间的，招聘、过滤、磨合，尤其是一个好的leader，对各项素质要求都不低：工程、技术、管理、视野都少不了，有些可遇不可求的意味。

想让整个流程走起来，工具开发、经验积累也需要时间。 

  


* **数据采集、标注、管理**。

这个得看场景，采集设备便宜的话，成本倒还好说；贵的话，一台采集设备就可以顶半个房间的训练硬件了，采集规模要上去的话，会觉得DGX-2的价钱和iPhone XR的黑边一样，越来越美好了。

这活还少不了费时间和心力，设备标定、标准制定、流程磨合，这些都是没法避免的。从头开始的话，采集系统比较复杂的，比如类似KITTI这样子的，贯彻996精神，怎么也得是半年起步了（其实我想把起步价说成年，但是人定胜天、福报万岁）。

数据到了还要管理和标注，洗数据、标数据、验证和测试、存储入库，这也需要不断的迭代，所以，数据型的公司在这件事情上是有先天的优势。当然，现在开放的数据也不少了，可以先期用开放数据跑一跑，验证技术和模型是没问题的。

**另外，别人家公司的私有数据，可能他们公司脑子抽了才会公开给你用。 何况大家对外公开宣称的数据量，和统计局的GDP增长率一样，迷的很。**

  


所以，我觉得，归根结底，三要素里还是数据宝贵。

