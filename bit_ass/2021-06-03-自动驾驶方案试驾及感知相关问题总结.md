---
layout: default
---
# 自动驾驶方案试驾及感知相关问题总结

 [*Link:*](https://zhuanlan.zhihu.com/p/377643912)

最近试驾了两台车的自动驾驶，一个新势力，一个传统主机厂。刚好可以从半个从业者的角度看看视觉感知的问题。 

传感器上，都是依靠 毫米波+视觉 的思路。 从视觉的角度来看基本功能主要两个: 

1. 目标检测
2. 车道检测

## 暴露问题  
  


**目标检测**  
   
**1. 识别不连续的情况偶有发生**

这个问题从目标检测的情况下是难以避免的，都是靠跟踪滤波算法尽量让目标稳定。不过，这会加重下面识别延迟的问题。 

  
想要尽量缓解这个问题，除了最粗暴的刷指标，可以从多帧融合的角度来考虑（不单是网络结构和预测阶段考虑，训练阶段也可以利用多帧之间的关系）。   
多相机之间目标进出也可能带来不稳定，这个可以通过多相机的融合，让神经网络可以进一步扩宽视野，让目标在更大的角度、更远的距离上被捕获。   
   
   
**2. 识别速度不足，速度较快的进入识别范围的车辆会有可以感知到的延迟**

识别延迟的问题来源可能有两个: 

* 本身的识别算法性能不足
* 跟踪、滤波算法对高速移动物体处理能力不足

现在的量产车似乎一直在强调自己的摄像头像素xxx万像素，比如现在的PPT一般都在8m像素。然而，实际上从自动驾驶感知的技术角度来看，在满足了一定像素后，高帧率相机配合高帧率算法才是一个更加务实的选择。   


对于像素，可以很简单的估计一个相机系统数据带宽。一般自动驾驶的车辆都会布置 8+ 相机，如果是8m像素，那么大致相当于 8路以上的 4K 视频的带宽。对于感知算法来说，这个分辨率基本都是浪费，算力支撑不起来在这个像素水平上的算法。 

即使可以用满带宽，车载算力平台也可能无法保证所有相机的稳定帧率。与其如此，还不如换个帧率稍高、像素稍低的（毕竟8m相机很有可能只有20FPS）相机，在算法性能上再考虑优化。 

  
   
**3. 主要限定在某些特定场景上，场景外的情况表现都明显有下滑**

这个和数据采集相关，不多说。当然，限定场景也是因为目前网络很难解决长尾问题，只能通过限定区域来减少长尾案例的出现概率。   
   
  **4.** **大型车辆近距离识别问题**

本身确实是一个难点，尺寸过大带来摄像头无法看到完整物体。尤其是大卡车，侧向的摄像头可能只能看到车身纹理信息了。虽然这种情况也可以检出有物体存在，但是由于缺少边缘，会无法判断物体的尺度、距离。给下游的决策和控制带来难度。   
这里可以尝试的也只能是通过扩大摄像头的视角范围，尽量拼凑出物体的边缘。或者折中，尽量获得地面信息，这样可以更加容易判断大车的运动趋势。   
   
   
**5. 拥堵的时候，跟车目标判断上会有混乱情况发生**

难解问题，拥堵之后会缺少车道线信息，很难判断那辆车是本车道内。尤其是，面对并入车道、弯道场景，外界的车辆轨迹在变，本车的车头方向也可能有变化。这个之前自己做ADAS的时候，也遇到过前车判断逻辑会容易跳目标。   


不过这个问题一般发生在慢速情况下，所以即使有跳变问题也不大。   
   
   
   
**车道检测**  


最明显的：不能处理匝道、弯道、路口等等车道线比较异常的情况。

  
这个可以说是场景限制带来的。实际上是，在这些案例上很难做好，只能放弃。而且，目前的很多车道线方案本身就不具备解决这些场景的能力。包括试乘的新势力的车，明显的不能处理横向车道的能力，所以完全不具备路口能力。 

  
车道检测的问题依靠逆向来猜测感知能力相对比较难，目前已知的也就是这个比较明显。 

## Tesla FSD  
FSD 在油管上已经有很多车主的视频了，相对比来看，Tesla确实是量产目标里面做的最全面的一个了。尤其是考虑 Tesla 的传感器/地图方案的激进，能做到目前的水平已经是难度很大了。因为各种原因，FSD 似乎被非议比较多，但是目前进度上，从量产目标角度看，Tesla其实是走的最远的。   
   
 

