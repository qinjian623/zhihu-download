---
layout: default
---
# Google AI 的一篇向量搜索相关的论文

 [*Link:*](https://zhuanlan.zhihu.com/p/165507483)

免责声明：我是在 PPT拖延症 的时候看的文章，也没怎么仔细看，可能会有遗漏。

  


*Accelerating Large-Scale Inference with Anisotropic Vector Quantization*

一篇和向量搜索相关的论文，思路和我自己之前暴力搜索优化的想法很类似

  


[Captain Jack：向量搜索短期工作总结](https://zhuanlan.zhihu.com/p/164955171)也是觉得向量搜索里面很多距离远的点意义不大。本文是在量化时对 reconstruction loss 加权，让距离太远的点对 loss 的影响小一点。

## 文中的思路  
在向量最近邻搜索中，那些距离太大（or IP 太小）的值实际上作用不大，所以量化过程中，可以搞一个加权。所以，虽然文章篇幅很长，但是千言万语汇成一句话：

​ $\ell = \mathop{\mathbb{E}}_{q \in Q}[w(\langle q, x_i\rangle) \langle q, x_i - \hat{x}_i \rangle] $ 

后面的一些证明也只是为了说明：

1. 这样的 Loss 会更好的贴合对应的 query
2. 之前传统的 reconstruction loss 只是针对重建的量化误差优化，但是新版本 loss 更加针对 End2End 的业务指标，扔掉那些没什么作用的 ​ 反而效果更好（文中的实验实际用的是 $w(t) = \mathbb{I}(t \ge T) $ ​ 这个，其实就是加权函数阈值过滤）。

## 问题  
1. 文中的 Loss 实际上引入了 query，如果后续实验里面，使用了测试用的query，实际上是数据污染，相当于封闭测试。这样的话，对比实验就不公平了。文中提到会用 uniform 的分布来模拟 ​ $Q$ .
2. Query 的分布虽然可能有统计学意义，但是按照这样的加权，会不会牺牲小众 query？
3. 由于 query 本身的不可预知，所以之前自己的工作才选择先用蒙特卡洛采样 query 的分布特征，不过后续的 NN-everything 规划，也会面临这个问题，训练的 query 会带来一些先验的 bias。

## Ref:  
1. [https://ai.googleblog.com/2020/07/announcing-scann-efficient-vector.html](https://ai.googleblog.com/2020/07/announcing-scann-efficient-vector.html)
2. [https://arxiv.org/abs/1908.10396](https://arxiv.org/abs/1908.10396)

