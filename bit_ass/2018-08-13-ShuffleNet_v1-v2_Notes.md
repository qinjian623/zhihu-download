---
layout: default
---
# ShuffleNet v1-v2 Notes

 [*Link:*](https://zhuanlan.zhihu.com/p/41914759)

上月月底读过一段时间了，今天再复习一下，简单记录一下。  


ShuffleNet用于低性能硬件的网络结构，似乎一直对标的Google的MobileNet。基本上每次都要提一提MobileNet，对比一番。  


目前对于小网络的基本思路就是

1. 增加group，这样可以降低计算量，降低模型复杂度（在输出输出的channels和大网络一致的前提一下。）
2. 压缩与扩展网络，就是bottleneck结构。

当然，整体来说随着网络复杂度变小，整个网络的指标肯定是在下降的，又快又好的网络基本不存在（不讨论硬件的变化）。  




---

**V1**  


**Channel Shuffle**

V1首先还是基于上面说的基本思路，首先就是bottleneck的结构外加3x3的DWConv。想要在则这个结构的基础上进一步增加性能的话，可以看到bottleneck结构中的1x1 Conv也可以增加group（在ResNeXt的文章中也说明了，由于使用了group，在保持模型复杂度相同的情况下，是可以增加模型的性能的）。所以，随着1x1 Conv增加了group，就给了增加featuremap的空间。

不过如果1x1Conv也使用group的话会造成一个结果，整个网络被分裂开来，只是一个个单独的channel的信息一传到底，这和几个超级小的网络独立训练后做ensemble基本等价了。所以V1就使用了一个Channel Shuffle的方法让分裂的网络重新建立联系。

具体的内容就是对channel进行重新排布，让原先的分裂的网络重组。如果把数据看作水流，那么就是之前是好几路路水管每一路都一通到底，现在，就是一路粗水管到了一个地方，分成几个小水管，之后这些小水管会和别的粗水管分出的小水管混合在一起，重新组成一个新的粗水管。这样的话，就达到了一个目的：水管不再是一通到底，而是不断的交汇分开。  


**Concat**  
V1另外一个的变化就是在stride=2的基本结构中，没用使用residual的add，而是在将之前的feature通过AvgPooling短路到下层，同时不再是使用add操作，而是concat。这个结构目的是为了增加channel的同时可以减少计算量，使用AvgPooling的选择我猜测也是试验后的结果，这个选择可能比其他的down sample方法更好。  


**Experiments**

V1和V2论文的特点就是试验丰富，作者做了很多对比试验，所以上面的这些方法就变得比较坚实。

1. 1x1的group的参数对小网络能有更大的作用。
2. shuffle之后效果明显。
3. 与MobileNet对比也是基本都处于优势。

  




---

**V2**  


**设计准则**

之前单纯的评价FLOPs具有很大的局限性，同样FLOPs的网络在实际的硬件平台上的性能表现会有不同，所以论文认为除了FLOPs这个评价指标，还要考虑下面两个指标：

1. 内存读取的开销
2. 网络并行能力

在此基础上，提出了四个设计准则：

1. 相同输入输出channels的conv内存读取开销更小。这个和bottleneck的设计有了很大的冲突。
2. 同样的FLOPs下，group越多内存开销也越大。
3. 过于复杂的结构会降低网络的并行度。这对于GPU上的运行有明显的影响，CPU的影响相对较少，我猜测是CPU本身的运算模式相比GPU并没有带大并行度，并不能利用好并行度高的网络设计。
4. element-wise的操作也会显著影响性能。

文章其中的内存开销的试验对比可以看出，一般都是对GPU的影响更大，对CPU的影响很小。我的分析是CPU本身的运行速度要比GPU慢两个数量级，数据吞吐量并没有显露出来。GPU不同，运行速度太快，而且GPU的内存设计布局也不一样，需要更经常的数据交换，没注意文中的数据时间包不包括本地内存和显存的互相拷贝时间，这个如果网络的运行速度快了，应该也是很明显的。所以，其中对内存开销（MAC）的分析，对于CPU上的设计影响要小很多，毕竟也就是一秒几十次，而且还有CPU cache抗一部分（主要的就是G1，G2）。  


**结构**

1. 类似V1，但是其中1x1 Conv取消了group，单纯在这里已经可以不是用channel shuffle了。
2. 对输入做了split，一部分经过1x1 Conv -> 3x3 DWConv -> 1x1 Conv这一系列处理，一部分直接短接到后面。
3. 将处理后的feature与短接的输入concat在一起，之后再加上channel shuffle。这里使用channel shuffle的原因可能是由于前面的split的原因，同样也会造成V1一样的网络的分裂。
4. 对于down sample的方法，不再使用之前的Avg Pooling，而是利用stride=2的3x3的DW Conv。

这样的结构，就反应了前面的四个设计准则。后面的试验也是类似V1，够丰富，后面的Table8能填满就够一阵子工作量了。

先这样吧。。。

