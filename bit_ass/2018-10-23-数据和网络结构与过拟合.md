---
layout: default
---
# 数据和网络结构与过拟合

 [*Link:*](https://zhuanlan.zhihu.com/p/47478334)

*Understanding Deep Learning Requires Rethinking Generalization*

借着这篇文章里面的随机数据实验来说说。  


这篇文章当时出现的时候就争议很大，我自己读后也没有太关注，毕竟这个和日常的工作没多大关系。DL的模型到底里面什么个情况，没人能解释的通，但是效果好是实实在在看得到了。  


其中比较有意思的自然是随机测试的几个结果，文中的意思是NN会直接通过粗暴记忆结果来完成学习。但是，必须清楚的是，这个结论的前提：**随机测试。**  


自己重读这块的话，文中随机测试的结果，支持自己的这些人生教训：

1. 高质量的数据有助于提升网络表现
2. NN的结构在设计时，需要考虑贴合数据集合的特点

  


**数据问题**

首先，NN的训练只是单纯的去拟合训练数据，并不会去考虑训练数据本身逻辑上的合理性。你提供一个随机的数据集，NN就奔着这个随机的结果去拟合。总结来看，还是那句话，“rubbish in, rubbish out”。

事实上，论文中的随机数据训练并没有太多的支持力，NN一般都是过量参数的，自然也就具备了强制记忆训练数据的能力，但是不代表NN有了这个能力就一定会使用这个能力。重要的还是看训练过程中对网络的引导方向。

如果是随机数据，那么NN完全没有能够学习数据的规律和特征的机会。优化过程又会推着网络去缩小training error，那么唯一的解决方案就是：我把训练数据全背下来。

如果是正常数据，NN则可以通过学习有效的规律和特征来获得training error的降低。而且，从经验上来看，NN会优先去学习数据的特征，实在不行了才会开始背数据，原文的收敛速度上看，也是符合我们这个经验的。所以，背数据对NN来说也是不得已而为之的结果，你揠苗助长，那么只会逼着网络过拟合、背数据。

所以，从随机数据测试的角度看，在NN的应用上来说，高质量的数据就是对抗过拟合的最好手段（当然，谁都想更少的数据训练更佳的模型）。  


**NN结构设计**

NN的结构设计是需要贴合本身的数据集合特点的。最基本的原则就是：逻辑合理，利于辅助NN学习。

比如一个检测竖条纹比较关键的数据集，那么可以考虑选择扁一点的filter，这样NN参数的利用率可能更高。图中目标占比特别大的数据集，深层的感受野就需要够大，否则只能看到目标的局部，还要求猜的准，典型的揠苗助长。

虽然可以大才小用，比如直接YOLO finetune大概率小目标的数据集。理想情况下是网络的一部分没有得到训练，这部分的参数可能不可靠；不理想的情况可能就是容易过拟合。

NN的结构调整到适合于数据集，本身也是防止过拟合的一个手段。

