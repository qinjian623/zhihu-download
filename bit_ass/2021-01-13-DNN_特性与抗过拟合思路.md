---
layout: default
---
# DNN 特性与抗过拟合思路

 [*Link:*](https://zhuanlan.zhihu.com/p/343963913)

补年底KPI。

## A. DNN 的一些特点  
## 1. Overparameterization 与 W 曲线  
这是 NN 与一般 ML 方法不同的地方，增加模型的 capacity 甚至有可能增加泛化能力。

传统模型会随着模型的复杂度表现出 U 形曲线，随着模型复杂度的增加，模型的 test error 会先降低后增加。

NN 模型在初期同样有这个现象，但是随着模型复杂度的进一步增加， test error 会重新开始下降。这篇文章有很好的讨论：

[Are Deep Neural Networks Dramatically Overfitted?](https://lilianweng.github.io/lil-log/2019/03/14/are-deep-neural-networks-dramatically-overfitted.html)可能原因：更多参数，会让模型更加平滑。而对 **增加泛化性来说，平滑是一个是否重要的特性。**

为什么传统方法没有这个特点，我猜想是深度的增加带来的下面这个特性。同时现在的模型训练的 epoch 也变得越来越大了，因为大家发现多训练点时间也没什么过拟合。

## 2. 低->高频率的学习过程  
由于深度的的增加， NN 的学习是从印象到细节的过程。很类似人类的思维过程，回想桌子的第一反应是大概的样子，需要进一步细节来区分不同桌子的时候才会开始关注细节。

比如，知乎上有相关的讨论：

[如何从频域的角度解释CNN（卷积神经网络）？](https://www.zhihu.com/question/59532432)同时，这也引出了下面的一个能力对泛化的影响。

## 3. 强制记忆能力  
*Understanding deep learning requires rethinking generalization* 这篇经典文章说明了这个现象。

NN 巨大的模型容量，可以直接强制记忆整个数据集。如果使用一个完全没有逻辑的数据集进行训练，NN 会最终依靠来自细节的学习（因为抽象的概念表达不了已经乱序的label），最终网络的学习结果会是这样：“一个具备 xxx 细节的图像（实际可能是6），预测结果为 1。”

所以，**在训练的过程中，必须避免 NN 进入这种强制记忆的学习过程中**，此时NN 已经依靠自己容量，暴力拟合训练数据了。



---

## B.抗过拟合的思路与方法  
## 1. 避免强制记忆  
* *模型的设计必须能够匹配训练任务的逻辑*
+ 比如，一个需要关注图片细节的任务，网络设计上底层非常快的 downsample 图像，这样的网络更容易进入强制记忆（过拟合）的区间。

* **允许训练集合的一些错误**
+ 你不能保证网络的结构设计能够完美的适应任务，有些错误可能就是这个原因带来的，无脑的强制要求这些不犯错，网络只能靠背结果了。

**避免强制记忆方法主要靠训练前的数据分析+训练后的错误分析（这个准备再开新坑）**

## 2. 关注数据分布以及变化  
在训练前的数据分析中要关注数据分布和变化情况，比如：

* 数据的统计特点是否会随着时间变化？
+ 一个特征上个月和这个月的变化区间都不一样，那么这个特征就需要考虑如何人工进行特征工程，或者干脆扔掉。

* train/val/test的数据集合划分是否合理？
+ 训练集合大部分白天图像，测试集合大部分夜间图像。

* 数据类别的分布
+ 如果有分布不平衡的情况，需要考虑有更好的sampler、数据增强（小目标多拷贝几份在图上）、Loss设计等等。

## 3. 更多的数据  
* 更多更丰富的数据其实是 NN 训练的关键。
* 传统的数据增强，但是这里同样要防止网络强制记忆，因为数据增强也仅仅是在有限的数据集上进行变化。
* 业务指标与数据规模预估（这个准备再开新坑）

数据积累带来的新的训练模式：随着采集数据量增加vs标注产量的限制矛盾，加上最近学界新的成果，未来的训练模式肯定会有更多的变化。（这个准备再开新坑）

## 4.特征工程  
这个范围很大，基本都是任务强相关，比如：

* 是否有更好的特征可以使用（比如，Position Encoding一样可以用在 cnn 上）
* 特征是否对任务更加适合（未必一定要使用 RGB 的色彩空间、未必需要用自然比例的图片）
* 有些特征是否会有负面影响（比如，某些激光点云映射后的图片有很多固定 的区域是无穷远，参与训练没有意义）

## 5. 增加平滑性  
几乎任何可以增加平滑性的方法都能够让 NN 更好的学习。比如：

* 针对 weight/featmap： Batch Norm 以及各种 Norm 的变体，Regularization(weight decay)，不同迭代间的 weight 的加权平均(时序关系上的平滑)
* 针对 label： Label Smooth，Distill（用一个 teacher 给的平滑版本 label）
* 针对 loss：比如经典的 Smooth L1
* 针对 grad：经典的 MiniBatch SGD（可以认为是对 grad 的平均，也可以进一步做针对 grad 的 norm），对 grad 的平滑（比如 Momentum）
* 针对 数据/featmap：各种 preprocess 的手段，比如 高斯平滑/whiten/SVD、多个数据加权叠加（实际上输入数据和 featmap 本质上是一个东西，所以，所有针对数据的操作都可以针对 featmap）

关于 featmap 和数据的关系，比如：输入的 RGB 数据，实际上不过是 CMOS 硬件转换出来后的 3 channel 的 featmap。所以，所有对 featmap 的操作都可以用在输入数据上，反之亦然。

额外的，如果对 featmap 的操作会产生针对 weight 的梯度，那么这些操作还会产生对 weight 的影响。

## 6. 随机性  
基本操作就是两项： drop（+ replace） or noise

对于 drop 后的值，也可以用各种方法再填充，比如 dropout 可以认为是用 0填充，但是也没规定不能用白噪音等等脑洞方式来填充，切一片其他图片贴进去也可以。

* 针对 输入数据： 各种数据增强的手段（可以认为是加 noise）
* 针对 featmap：dropout 还有现在的各种 drop 的变体（检测中针对目标区域、随机drop有空间关系的block，drop channel），加 noise 也可以
* 针对 weight，和针对 featmap 类似，但是俩者对学习时的 grad 影响不一样。
* 针对 grad：加 noise 也算是传统操作了，不知道有没有针对 grad 搞drop的。

  


针对 featmap 的操作思路，这些都可以用在 *随机性* 和 *平滑性* 上：

* 纯随机
* 空间关系
* channel 关系
* 时序关系
* 数据集关系（比如 BN）

## 7. 传统手段  
* training: early stop
* constraints/regularization:
+ multi-task learning 之前我简单提了： [https://zhuanlan.zhihu.com/p/134109849](https://zhuanlan.zhihu.com/p/134109849)
+ l1/l2 （这个平滑性里面提了，不啰嗦了）
+ Transfer Learning(static & dynamics)
+ 数值统计特性限制

* ensemble/distill

