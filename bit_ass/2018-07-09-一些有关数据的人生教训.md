---
layout: default
---
# 一些有关数据的人生教训

 [*Link:*](https://zhuanlan.zhihu.com/p/39331845)

作为一个loser，人生经验是没有的，人生教训是一箩筐的～

  


使用Deep Learning容易带来对数据理解的忽视，似乎只需要足够的数据，DL就可以完成任务。然而，只是在某些任务上具有这个特性。其他的任务中，单纯的使用这样的粗暴方法，往往是不能够满足需要的。

  


对于CV和NLP的任务，确实，对数据的理解和质量的要求要比其他的任务低一些。ImageNet单纯的增加图片数量就可以获得非常大的改进，即使数据质量可能有高有低，但是只要不是太离谱，对最后指标的影响是不大的（当然，肯定有影响）。最近zhihu上好像刷到有人认为难度上CV<NLP<其他的一些任务，确实，起码在数据处理、特征提取和分析这方面，这排序是没毛病的。

  


但是，面对抽象的、已经结构化的、业务相关的这些特征，对数据的重视就显得十分重要。尤其是实际场景中，数据量可能并不是自己可控的，多少都得看天，数据的价值远胜对模型参数的调整。

  


所以如果条件具备，花一段时间亲手整理和熟悉数据是有必要的，看看数据分布、标注质量、整个数据集倾向，虽然这些都是dirty work没人愿意做，但是不做会吃亏的，会吃亏的，会吃亏的。在数据整理的过程中可能会发现：有些数据的质量和分布是不满足要求的，垃圾数据和无用数据都是需要过滤的，这些被污染的数据喂给模型吃，可以完全抵消辛苦调参一个周的工作。

同时，对于结构化的数据，需要对解决的问题有高度的理解（虽然图像、文本类任务对理解的要求要低一些，但是也是需要理解问题的），这样才能提出针对性的方法来解决问题。花费在数据整理以及后续的数据分析与特征工程的时间，理应超过模型训练、调参的人工时间（不包括机器时间，毕竟机器没人权）。起码在前期需要有这样的一个强制要求，否则，就尽想着瞎调参去了。好的数据、特征处理策略是远大于瞎调参的收益的。

  


考虑到机器是没有人权的，所以，运算性能是个硬基础。一个不能保证运算性能的DL/ML部门，是中世纪欧洲的部门。另外一个硬基础就是数据工具，有对特定问题针对性的数据展示、特征分析的工具是一个重要的支撑，大概率可以让效率翻翻，明智的个人/团队都应该有意识的分配资源投入这一方面的工作。

  


另外，目前为止，我依然对轻微过拟合所有偏爱。一般来说轻微过拟合时机下的模型的loss曲线是相对更加平滑的，因为模型已经在数据集上有了更好的磨合。最近有一篇论文讨论了Batch Norm优化的表现就在于更平滑的loss，当然，和偏爱过拟合没什么必然关系，但是反过来看，是不是更加平滑的loss的时候标志着网络对数据的学习更加成熟？毕竟，在网络训练初期，更加容易出现loss较大波动。

